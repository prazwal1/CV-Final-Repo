{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fcef87e-215d-4f82-9188-abcbd01f3ebe",
   "metadata": {},
   "source": [
    "# UNET IMPLEMENTATION ON ROAD SEGMENT DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f661ee00-2616-429e-a0f2-41fe3b2939ed",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c2df8d0-ce9c-40e4-a1e8-0a5089eb951d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_snippets import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models import vgg16_bn\n",
    "import cv2 as cv\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3682d98e-6f10-4782-8719-b8ee8404188b",
   "metadata": {},
   "source": [
    "## Dataset taken from here: https://www.kaggle.com/datasets/sanadalali/satellite-images-for-road-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b76d5da-1540-4f07-ad10-d8c034b6a338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch: torch.Size([8, 3, 256, 256]) torch.Size([8, 1, 256, 256])\n",
      "Valid batch: torch.Size([8, 3, 256, 256]) torch.Size([8, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "class RoadSegDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_names = sorted(os.listdir(image_dir))\n",
    "        self.mask_names = sorted(os.listdir(mask_dir))\n",
    "        self.transform = transform or T.Compose([\n",
    "            T.Resize((256, 256)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        self.mask_transform = T.Compose([\n",
    "            T.Resize((256, 256)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_names[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_names[idx])\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        image = self.transform(image)\n",
    "        mask = self.mask_transform(mask)\n",
    "        mask = (mask > 0.5).float()  # binary mask 0/1\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "def get_dataloaders(image_dir, mask_dir, batch_size=8, val_split=0.2, seed=42):\n",
    "    dataset = RoadSegDataset(image_dir, mask_dir)\n",
    "\n",
    "    # Split sizes\n",
    "    val_size = int(len(dataset) * val_split)\n",
    "    train_size = len(dataset) - val_size\n",
    "\n",
    "    # Reproducible random split\n",
    "    torch.manual_seed(seed)\n",
    "    train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    train_loader, val_loader = get_dataloaders(\n",
    "        \"dataset/training/images\", \"dataset/training/groundtruth\",\n",
    "        batch_size=8,\n",
    "        val_split=0.2\n",
    "    )\n",
    "\n",
    "    for imgs, masks in train_loader:\n",
    "        print(\"Train batch:\", imgs.shape, masks.shape)\n",
    "        break\n",
    "\n",
    "    for imgs, masks in val_loader:\n",
    "        print(\"Valid batch:\", imgs.shape, masks.shape)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fd2af61-e0e6-43ad-adc6-b3849be493bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2\n"
     ]
    }
   ],
   "source": [
    "# Choose device - change (cuda:2 --> cuda) in case you dont have multiple gpu\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58e27108-0625-4f4d-b6ee-edf295f532dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Params dictionary\n",
    "class Params(object):\n",
    "    def __init__(self, batch_size, test_batch_size, epochs, lr, seed, cuda, log_interval):\n",
    "        self.batch_size = batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.seed = seed\n",
    "        self.cuda = 'cuda:2' if cuda and torch.cuda.is_available() else 'cpu'\n",
    "        self.log_interval = log_interval\n",
    "\n",
    "# Configure args\n",
    "args = Params(8, 2, 5, 1e-3, 1, True, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d43cbb-432d-4ddc-b889-544fa78fb7bf",
   "metadata": {},
   "source": [
    "# U Net Architecture\n",
    "\n",
    "- U-Net is a convolutional neural network architecture primarily used for image segmentation tasks. It consists of a contracting path (encoder) that captures context and a symmetric expanding path (decoder) that enables precise localization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd3854-084f-40ab-8bb2-94e5e4cdaf3a",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "### Initializes the U-Net model.\n",
    "   \n",
    "Takes two parameters:\n",
    "\n",
    "    - pretrained: A boolean indicating whether to use a pretrained VGG16 model.\n",
    "    - out_channels: The number of output channels for the final segmentation map (e.g., 12 classes for segmentation).\n",
    "    \n",
    "### Encoder (Contractive Path)\n",
    "\n",
    "    - The encoder part uses the features from a VGG16 model with batch normalization (vgg16_bn).\n",
    "    - The encoder is divided into five blocks, each consisting of several convolutional layers that progressively reduce the spatial dimensions while increasing the number of feature channels.\n",
    "    \n",
    "## Bottleneck\n",
    "\n",
    "    - The bottleneck section takes the deepest layers of the encoder.\n",
    "    - A convolution layer `conv_bottleneck` is applied to increase the number of feature channels from 512 to 1024, allowing the network to learn more complex features.\n",
    "    \n",
    "\n",
    "## Decoder (Expansive Path)\n",
    "\n",
    "The decoder consists of up-convolution (or transposed convolution) layers followed by concatenation with corresponding encoder features to retain spatial information:\n",
    "\n",
    "    - Each up_conv layer increases the spatial dimensions (upsampling).\n",
    "    - The output of each up-convolution is concatenated with the corresponding feature map from the encoder (skip connections).\n",
    "    - This helps the model learn both high-level features from deeper layers and low-level features from shallower layers.\n",
    "    - Finally, conv11 reduces the number of channels to out_channels (e.g., for multi-class segmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ebf39b-ea57-4e4c-a32b-4e715ede5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bde56da-5e40-4dac-bece-50357e32dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73cb9c0b-cb02-40e7-a16c-fa9e75ae1965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, pretrained=True, out_channels=12):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = vgg16_bn(pretrained=pretrained).features\n",
    "        self.block1 = nn.Sequential(*self.encoder[:6])\n",
    "        self.block2 = nn.Sequential(*self.encoder[6:13])\n",
    "        self.block3 = nn.Sequential(*self.encoder[13:20])\n",
    "        self.block4 = nn.Sequential(*self.encoder[20:27])\n",
    "        self.block5 = nn.Sequential(*self.encoder[27:34])\n",
    "\n",
    "        self.bottleneck = nn.Sequential(*self.encoder[34:])\n",
    "        self.conv_bottleneck = conv(512, 1024)\n",
    "\n",
    "        self.up_conv6 = up_conv(1024, 512)\n",
    "        self.conv6 = conv(512 + 512, 512)\n",
    "        self.up_conv7 = up_conv(512, 256)\n",
    "        self.conv7 = conv(256 + 512, 256)\n",
    "        self.up_conv8 = up_conv(256, 128)\n",
    "        self.conv8 = conv(128 + 256, 128)\n",
    "        self.up_conv9 = up_conv(128, 64)\n",
    "        self.conv9 = conv(64 + 128, 64)\n",
    "        self.up_conv10 = up_conv(64, 32)\n",
    "        self.conv10 = conv(32 + 64, 32)\n",
    "        self.conv11 = nn.Conv2d(32, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Contractive Path\n",
    "        block1 = self.block1(x)\n",
    "        block2 = self.block2(block1)\n",
    "        block3 = self.block3(block2)\n",
    "        block4 = self.block4(block3)\n",
    "        block5 = self.block5(block4)\n",
    "\n",
    "        bottleneck = self.bottleneck(block5)\n",
    "        x = self.conv_bottleneck(bottleneck)\n",
    "        # Expansive Path\n",
    "        x = self.up_conv6(x)\n",
    "        x = torch.cat([x, block5], dim=1)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        x = self.up_conv7(x)\n",
    "        x = torch.cat([x, block4], dim=1)\n",
    "        x = self.conv7(x)\n",
    "\n",
    "        x = self.up_conv8(x)\n",
    "        x = torch.cat([x, block3], dim=1)\n",
    "        x = self.conv8(x)\n",
    "\n",
    "        x = self.up_conv9(x)\n",
    "        x = torch.cat([x, block2], dim=1)\n",
    "        x = self.conv9(x)\n",
    "\n",
    "        x = self.up_conv10(x)\n",
    "        x = torch.cat([x, block1], dim=1)\n",
    "        x = self.conv10(x)\n",
    "\n",
    "        x = self.conv11(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71c184e7-6c3b-4236-893e-3e134d8b0fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torchvision.models.vgg.vgg16_bn(*, weights: Optional[torchvision.models.vgg.VGG16_BN_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vgg.VGG>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fef1a14-c584-4f59-ae6a-0feb71c80b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ce = nn.CrossEntropyLoss()   # Applies softmax to output logits --> converts into class probabilities --> calculates neg. log likelihood loss between pred and true class label!!\n",
    "\n",
    "def UnetLoss(preds, targets):\n",
    "    targets = targets.squeeze(1).long()   # <-- remove the channel dimension\n",
    "    ce_loss = ce(preds, targets)\n",
    "    acc = (torch.max(preds, 1)[1] == targets).float().mean()\n",
    "    #  (torch.max(preds, 1)[1] returns the indices of the maximum values along the class dimension (i.e., the predicted class for each pixel). \n",
    "    #  The 1 indicates that we're looking along the columns (the class dimension).\n",
    "    #  if preds class == targets return 1 --> change to float --> take mean to keep score between 0 and 1\n",
    "    return ce_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9aea8bb-8d15-4a8a-a3b7-f136bb65c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainEngine():\n",
    "    def train_batch(model, data, optimizer, criterion):\n",
    "        model.train()\n",
    "        for imgs, masks in data:\n",
    "            ims, ce_masks = imgs.to(device), masks.to(device)\n",
    "            \n",
    "        _masks = model(ims)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, acc = criterion(_masks, ce_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss.item(), acc.item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate_batch(model, data, criterion):\n",
    "        model.eval()\n",
    "        for imgs, masks in data:\n",
    "            ims, ce_masks = imgs.to(device), masks.to(device)\n",
    "            \n",
    "        _masks = model(ims)\n",
    "\n",
    "        loss, acc = criterion(_masks, ce_masks)\n",
    "\n",
    "        return loss.item(), acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "845967f7-fa7f-461c-abed-1ed748f6c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "def make_model():\n",
    "    model = UNet().to(args.cuda)\n",
    "    criterion = UnetLoss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f025fdad-495f-4d49-942f-c434ae165bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jupyter-dsai-st123439/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num. of parametes: 29311011\n",
      "Total num. of Trainable parametes: 29311011\n"
     ]
    }
   ],
   "source": [
    "model, criterion, optimizer = make_model()\n",
    "# Total num. of parametes\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "# Total num. of \"trainable\" parameters\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total num. of parametes: {num_params}')\n",
    "print(f'Total num. of Trainable parametes: {num_trainable_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68b4372a-41e0-44f1-a516-f218766e47d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model():\n",
    "    for epoch in range(args.epochs):\n",
    "        print(\"####################\")\n",
    "        print(f\"       Epoch: {epoch}   \")\n",
    "        print(\"####################\")\n",
    "\n",
    "        for batch_idx, data in tqdm(enumerate(train_loader), total=len(train_loader), leave=False):\n",
    "            train_loss, train_acc = TrainEngine.train_batch(model, train_loader, optimizer, criterion)\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                # Print training information inline instead of calling a function\n",
    "                step = epoch * len(train_loader) + batch_idx\n",
    "                print(f'Epoch [{epoch+1}/{args.epochs}], Step [{batch_idx}/{len(train_loader)}], '\n",
    "                      f'Train Loss: {train_loss:.6f}, Accuracy: {train_acc:.6f}')\n",
    "\n",
    "        avg_val_acc = avg_val_loss = 0.0\n",
    "        for batch_idx, data in tqdm(enumerate(val_loader), total=len(val_loader)):\n",
    "            val_loss, val_acc = TrainEngine.validate_batch(model, val_loader, criterion)\n",
    "\n",
    "            avg_val_loss += val_loss\n",
    "            avg_val_acc += val_acc\n",
    "\n",
    "        step = (epoch + 1) * len(train_loader)\n",
    "        avg_val_loss /= len(val_loader)\n",
    "        avg_val_acc /= len(val_loader)\n",
    "        print(f'Val: Average loss: {avg_val_loss:.4f}, Accuracy: {avg_val_acc:.4f}')\n",
    "        print()\n",
    "\n",
    "    # Save the model and optimizer states after training is complete\n",
    "    # torch.save({\n",
    "    #     'model_state_dict': model.state_dict(),\n",
    "    #     'optimizer_state_dict': optimizer.state_dict()\n",
    "    # }, 'unet.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0af5080-3d59-496c-b4f7-d6a100c5f0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "       Epoch: 0   \n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:17,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [0/10], Train Loss: 1.040541, Accuracy: 0.454958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.47it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: Average loss: 1.0813, Accuracy: 0.2351\n",
      "\n",
      "####################\n",
      "       Epoch: 1   \n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:12,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [0/10], Train Loss: 0.667452, Accuracy: 0.864990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.62it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: Average loss: 1.0531, Accuracy: 0.3124\n",
      "\n",
      "####################\n",
      "       Epoch: 2   \n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:11,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [0/10], Train Loss: 0.504151, Accuracy: 0.922363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.64it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: Average loss: 0.6757, Accuracy: 0.8655\n",
      "\n",
      "####################\n",
      "       Epoch: 3   \n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:13,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [0/10], Train Loss: 0.433789, Accuracy: 0.937992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.84it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: Average loss: 0.3127, Accuracy: 0.9373\n",
      "\n",
      "####################\n",
      "       Epoch: 4   \n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:12,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [0/10], Train Loss: 0.356298, Accuracy: 0.929749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: Average loss: 0.2798, Accuracy: 0.9345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d22232f-9307-42dc-b5fe-7d4e3c2c61dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for bx, (imgs, masks) in tqdm(enumerate(val_loader), total=len(val_loader)):\n",
    "        ims, ce_masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        preds = model(ims)\n",
    "        preds = torch.max(preds, dim=1)[1]  # predicted class per pixel, shape [B,H,W]\n",
    "\n",
    "        # move to CPU for visualization\n",
    "        img_cpu = ims[0].permute(1, 2, 0).cpu()        # RGB image [H,W,3]\n",
    "        mask_cpu = ce_masks[0, 0].cpu()                # Ground truth [H,W]\n",
    "        pred_cpu = preds[0].cpu()                      # Prediction [H,W]\n",
    "\n",
    "        # plot original RGB image\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img_cpu)  # RGB image\n",
    "        plt.savefig(f'original_image_{bx}.jpg')\n",
    "        plt.close()\n",
    "\n",
    "        # plot ground truth mask (grayscale)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(mask_cpu, cmap='gray')\n",
    "        plt.savefig(f'groundtruth_mask_{bx}.jpg')\n",
    "        plt.close()\n",
    "\n",
    "        # plot predicted mask (grayscale)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(pred_cpu, cmap='gray')\n",
    "        plt.savefig(f'predicted_mask_{bx}.jpg')\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6b5fde-e570-4a50-a71f-5f485f402b96",
   "metadata": {},
   "source": [
    "## IMPLEMENT SAM (SEGMENT ANYTHING MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36def362-6103-46bf-aeb7-4d4d2ac8221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Follow the blog post! - Choose a dataset to segment!\n",
    "\n",
    "## https://medium.com/@hasfatauil12/sam-segment-anything-model-d4f541165f6b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

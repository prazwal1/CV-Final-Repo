{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a6e235-3513-405b-bbda-56d71ce87bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conv_output_size(\n",
    "        input_size,   # (H, W)\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        dilation=1\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Computes output (H, W), output channels, and total features after a Conv2D-like layer.\n",
    "    Supports int or tuple arguments.\n",
    "    \"\"\"\n",
    "    # unpack input H, W\n",
    "    H, W = input_size\n",
    "\n",
    "    # allow int or tuple kernel/stride/paddisng/dilation\n",
    "    def to_tuple(x):\n",
    "        return (x, x) if isinstance(x, int) else x\n",
    "\n",
    "    KH, KW = to_tuple(kernel_size)\n",
    "    SH, SW = to_tuple(stride)\n",
    "    PH, PW = to_tuple(padding)\n",
    "    DH, DW = to_tuple(dilation)\n",
    "\n",
    "    # compute output height and width using PyTorch formula\n",
    "    out_H = ((H + 2*PH - DH*(KH - 1) - 1) // SH) + 1\n",
    "    out_W = ((W + 2*PW - DW*(KW - 1) - 1) // SW) + 1\n",
    "\n",
    "    # output channels always = out_channels\n",
    "    out_C = out_channels\n",
    "\n",
    "    # total number of features\n",
    "    total_features = out_C * out_H * out_W\n",
    "\n",
    "    return (out_C, out_H, out_W, total_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e08dfdd-863e-427f-a3ce-13a495ab5328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30, 30, 9000)\n"
     ]
    }
   ],
   "source": [
    "print(compute_conv_output_size(\n",
    "    input_size=(32, 32),\n",
    "    in_channels=3,\n",
    "    out_channels=10,\n",
    "    kernel_size=3,\n",
    "    stride=1,\n",
    "    padding=0\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c5c2d-dcea-4ff7-9347-7f9aed3cf487",
   "metadata": {},
   "source": [
    "# Loading Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3673d4-0bca-4e0b-a739-9d98b9582ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_pretrained_weights(model, state_dict_path=None, pretrained_model=None, strict=False):\n",
    "    \"\"\"\n",
    "    Load pretrained weights into a flexible model, skipping layers that don't match in shape.\n",
    "\n",
    "    Args:\n",
    "        model: nn.Module - your flexible model\n",
    "        state_dict_path: str - path to pretrained state_dict (optional)\n",
    "        pretrained_model: nn.Module - another model with pretrained weights (optional)\n",
    "        strict: bool - whether to enforce exact match for remaining layers (default False)\n",
    "    \"\"\"\n",
    "\n",
    "    if state_dict_path:\n",
    "        # Load weights from a saved checkpoint\n",
    "        pretrained_dict = torch.load(state_dict_path, map_location='cpu')\n",
    "        if 'state_dict' in pretrained_dict:\n",
    "            pretrained_dict = pretrained_dict['state_dict']\n",
    "    elif pretrained_model:\n",
    "        # Get state_dict from another model\n",
    "        pretrained_dict = pretrained_model.state_dict()\n",
    "    else:\n",
    "        raise ValueError(\"Either state_dict_path or pretrained_model must be provided.\")\n",
    "\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Filter out layers that don't match in shape\n",
    "    filtered_dict = {}\n",
    "    for k, v in pretrained_dict.items():\n",
    "        if k in model_dict:\n",
    "            if model_dict[k].shape == v.shape:\n",
    "                filtered_dict[k] = v\n",
    "            else:\n",
    "                print(f\"Skipping layer {k}: size mismatch {v.shape} vs {model_dict[k].shape}\")\n",
    "        else:\n",
    "            print(f\"Skipping layer {k}: not found in target model\")\n",
    "\n",
    "    # Load matched layers\n",
    "    model_dict.update(filtered_dict)\n",
    "    model.load_state_dict(model_dict, strict=strict)\n",
    "    print(f\"Loaded {len(filtered_dict)} / {len(model_dict)} layers from pretrained weights.\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787090f6-7943-4234-8b73-b285d8510d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Suppose we want to use AlexNet pretrained weights\n",
    "pretrained_alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "# Our flexible AlexNet with different input channels / classes\n",
    "alex_cnn = AlexNet(in_channels=1, num_classes=10)\n",
    "\n",
    "# Load pretrained weights where shapes match\n",
    "alex_cnn = load_pretrained_weights(alex_cnn, pretrained_model=pretrained_alexnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb48e1f-f781-42ca-afa8-3a515c64754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to checkpoint\n",
    "checkpoint_path = \"alexnet_cifar10.pth\"\n",
    "alex_cnn = load_pretrained_weights(alex_cnn, state_dict_path=checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71934ff-5a9a-462b-ad3e-043259a7f3fe",
   "metadata": {},
   "source": [
    "### Calculate the output size of 1D convolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ea6e67-0178-4fbf-ba0e-b0fab9e3cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_output_size(input_size, kernel_size, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Calculate the output size of a 1D convolution operation.\n",
    "\n",
    "    Formula:\n",
    "        output = floor((W - K + 2P) / S) + 1\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : int\n",
    "        The size of the input (W).\n",
    "    kernel_size : int\n",
    "        The size of the convolution kernel/filter (K).\n",
    "    stride : int, optional (default=1)\n",
    "        Step size with which the filter moves across the input (S).\n",
    "    padding : int, optional (default=0)\n",
    "        The number of padded zeros added to both sides of the input (P).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The computed output size after applying convolution.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If kernel_size is larger than the padded input size.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> conv_output_size(32, 3, stride=1, padding=1)\n",
    "    32\n",
    "\n",
    "    >>> conv_output_size(28, 5, stride=2, padding=0)\n",
    "    12\n",
    "    \"\"\"\n",
    "    # Calculate effective input size after padding\n",
    "    effective_input = input_size + 2 * padding\n",
    "\n",
    "    if kernel_size > effective_input:\n",
    "        raise ValueError(\"Kernel size cannot be larger than padded input size.\")\n",
    "\n",
    "    # Apply convolution formula\n",
    "    output = (effective_input - kernel_size) // stride + 1\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e1abd8c-d3c3-43ab-911d-4544789de196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(conv_output_size(32, 3, stride=1, padding=1))   # 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f1f1c8-6557-41a5-b69d-6b6bf2ebb3e2",
   "metadata": {},
   "source": [
    "### Calculate the convolutin Output with no padding or stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6816d97-a094-4999-b08a-78371d7bc88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_output_no_padding_no_stride(input_size, kernel_size):\n",
    "    \"\"\"\n",
    "    Compute the output size of a 1D convolution when:\n",
    "      - No padding (P = 0)\n",
    "      - No stride (S = 1)\n",
    "\n",
    "    Formula:\n",
    "        Output = W - K + 1\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : int\n",
    "        Size of the input (W).\n",
    "    kernel_size : int\n",
    "        Size of the convolution kernel/filter (K).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The output size after applying convolution.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If kernel_size is larger than the input_size.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> conv_output_no_padding_no_stride(10, 3)\n",
    "    8\n",
    "    \"\"\"\n",
    "    if kernel_size > input_size:\n",
    "        raise ValueError(\"Kernel size cannot be larger than input size.\")\n",
    "\n",
    "    return input_size - kernel_size + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21431e66-ff96-4fc0-b293-ea3ab58479c3",
   "metadata": {},
   "source": [
    "### Padding for same size as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "314c4142-49b3-49ec-a683-c1ab164ecd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_padding(kernel_size):\n",
    "    \"\"\"\n",
    "    Compute the required padding (P) for a convolution to produce\n",
    "    the same output size as the input (“same” convolution).\n",
    "\n",
    "    Formula:\n",
    "        P = (K - 1) / 2\n",
    "\n",
    "    Conditions:\n",
    "        - Works only when kernel_size (K) is odd.\n",
    "        - Stride must be 1 for exact same-size output.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kernel_size : int\n",
    "        Size of the kernel/filter (K). Must be an odd integer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The required padding amount P.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If kernel_size is not odd.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> same_padding(3)\n",
    "    1\n",
    "\n",
    "    >>> same_padding(5)\n",
    "    2\n",
    "    \"\"\"\n",
    "    if kernel_size % 2 == 0:\n",
    "        raise ValueError(\"Kernel size must be odd to achieve same output size.\")\n",
    "\n",
    "    return (kernel_size - 1) // 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1593e0e-c9b5-41f3-b040-5a275f0e8350",
   "metadata": {},
   "source": [
    "### Height and Width of 2D pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05c19afd-e9f5-45a2-a350-a1e97aff6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling_output_size(input_height, input_width, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    Compute the output height and width of a 2D pooling layer.\n",
    "\n",
    "    Formula:\n",
    "        H' = (H - K) / S + 1\n",
    "        W' = (W - K) / S + 1\n",
    "\n",
    "    Assumes:\n",
    "        - No padding (typical for pooling layers)\n",
    "        - Kernel size and stride are integers\n",
    "        - (H - K) and (W - K) must be divisible by S for valid output\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_height : int\n",
    "        Height of the input feature map (H).\n",
    "    input_width : int\n",
    "        Width of the input feature map (W).\n",
    "    kernel_size : int\n",
    "        Pooling kernel size (K).\n",
    "    stride : int\n",
    "        Stride of the pooling operation (S).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (int, int)\n",
    "        Output height H' and width W'.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If kernel size is larger than input dimensions,\n",
    "        or if the dimensions are not divisible by stride.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> pooling_output_size(28, 28, kernel_size=2, stride=2)\n",
    "    (14, 14)\n",
    "\n",
    "    >>> pooling_output_size(32, 32, kernel_size=4, stride=2)\n",
    "    (15, 15)\n",
    "    \"\"\"\n",
    "    if kernel_size > input_height or kernel_size > input_width:\n",
    "        raise ValueError(\"Kernel size cannot be larger than input dimensions.\")\n",
    "\n",
    "    if (input_height - kernel_size) % stride != 0:\n",
    "        raise ValueError(\"Input height is not compatible with stride.\")\n",
    "    if (input_width - kernel_size) % stride != 0:\n",
    "        raise ValueError(\"Input width is not compatible with stride.\")\n",
    "\n",
    "    out_height = (input_height - kernel_size) // stride + 1\n",
    "    out_width = (input_width - kernel_size) // stride + 1\n",
    "\n",
    "    return out_height, out_width\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693b5641-a8a2-4d3c-a78e-946b2ab6e57b",
   "metadata": {},
   "source": [
    "### Receptive Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b705afad-9bc0-444a-b18d-2423be01b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def receptive_field(kernel_size, num_layers):\n",
    "    \"\"\"\n",
    "    Compute the receptive field (RF) size of a stack of convolutional layers.\n",
    "\n",
    "    Formula:\n",
    "        RF = 1 + L * (K - 1)\n",
    "\n",
    "    Assumes:\n",
    "        - Stride = 1 for all layers\n",
    "        - All layers have the same kernel size\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kernel_size : int\n",
    "        Size of the convolution kernel/filter (K).\n",
    "    num_layers : int\n",
    "        Number of successive convolutional layers (L).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The receptive field size of the network.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> receptive_field(3, 1)\n",
    "    3\n",
    "\n",
    "    >>> receptive_field(3, 4)\n",
    "    10\n",
    "    \"\"\"\n",
    "    if kernel_size < 1:\n",
    "        raise ValueError(\"Kernel size must be at least 1.\")\n",
    "    if num_layers < 1:\n",
    "        raise ValueError(\"Number of layers must be at least 1.\")\n",
    "\n",
    "    return 1 + num_layers * (kernel_size - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e65d3c-da6c-48fc-a085-0808070e1e21",
   "metadata": {},
   "source": [
    "### Resize Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4288f1ac-efc8-47f5-8c6d-bdc9f901fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Upscale to AlexNet's expected size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ebc00-0a69-47cd-bd04-a3d0089ed3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

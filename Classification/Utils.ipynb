{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a6e235-3513-405b-bbda-56d71ce87bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conv_output_size(\n",
    "        input_size,   # (H, W)\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        dilation=1\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Computes output (H, W), output channels, and total features after a Conv2D-like layer.\n",
    "    Supports int or tuple arguments.\n",
    "    \"\"\"\n",
    "    # unpack input H, W\n",
    "    H, W = input_size\n",
    "\n",
    "    # allow int or tuple kernel/stride/paddisng/dilation\n",
    "    def to_tuple(x):\n",
    "        return (x, x) if isinstance(x, int) else x\n",
    "\n",
    "    KH, KW = to_tuple(kernel_size)\n",
    "    SH, SW = to_tuple(stride)\n",
    "    PH, PW = to_tuple(padding)\n",
    "    DH, DW = to_tuple(dilation)\n",
    "\n",
    "    # compute output height and width using PyTorch formula\n",
    "    out_H = ((H + 2*PH - DH*(KH - 1) - 1) // SH) + 1\n",
    "    out_W = ((W + 2*PW - DW*(KW - 1) - 1) // SW) + 1\n",
    "\n",
    "    # output channels always = out_channels\n",
    "    out_C = out_channels\n",
    "\n",
    "    # total number of features\n",
    "    total_features = out_C * out_H * out_W\n",
    "\n",
    "    return (out_C, out_H, out_W, total_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e08dfdd-863e-427f-a3ce-13a495ab5328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30, 30, 9000)\n"
     ]
    }
   ],
   "source": [
    "print(compute_conv_output_size(\n",
    "    input_size=(32, 32),\n",
    "    in_channels=3,\n",
    "    out_channels=10,\n",
    "    kernel_size=3,\n",
    "    stride=1,\n",
    "    padding=0\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c5c2d-dcea-4ff7-9347-7f9aed3cf487",
   "metadata": {},
   "source": [
    "# Loading Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3673d4-0bca-4e0b-a739-9d98b9582ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_pretrained_weights(model, state_dict_path=None, pretrained_model=None, strict=False):\n",
    "    \"\"\"\n",
    "    Load pretrained weights into a flexible model, skipping layers that don't match in shape.\n",
    "\n",
    "    Args:\n",
    "        model: nn.Module - your flexible model\n",
    "        state_dict_path: str - path to pretrained state_dict (optional)\n",
    "        pretrained_model: nn.Module - another model with pretrained weights (optional)\n",
    "        strict: bool - whether to enforce exact match for remaining layers (default False)\n",
    "    \"\"\"\n",
    "\n",
    "    if state_dict_path:\n",
    "        # Load weights from a saved checkpoint\n",
    "        pretrained_dict = torch.load(state_dict_path, map_location='cpu')\n",
    "        if 'state_dict' in pretrained_dict:\n",
    "            pretrained_dict = pretrained_dict['state_dict']\n",
    "    elif pretrained_model:\n",
    "        # Get state_dict from another model\n",
    "        pretrained_dict = pretrained_model.state_dict()\n",
    "    else:\n",
    "        raise ValueError(\"Either state_dict_path or pretrained_model must be provided.\")\n",
    "\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Filter out layers that don't match in shape\n",
    "    filtered_dict = {}\n",
    "    for k, v in pretrained_dict.items():\n",
    "        if k in model_dict:\n",
    "            if model_dict[k].shape == v.shape:\n",
    "                filtered_dict[k] = v\n",
    "            else:\n",
    "                print(f\"Skipping layer {k}: size mismatch {v.shape} vs {model_dict[k].shape}\")\n",
    "        else:\n",
    "            print(f\"Skipping layer {k}: not found in target model\")\n",
    "\n",
    "    # Load matched layers\n",
    "    model_dict.update(filtered_dict)\n",
    "    model.load_state_dict(model_dict, strict=strict)\n",
    "    print(f\"Loaded {len(filtered_dict)} / {len(model_dict)} layers from pretrained weights.\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787090f6-7943-4234-8b73-b285d8510d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Suppose we want to use AlexNet pretrained weights\n",
    "pretrained_alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "# Our flexible AlexNet with different input channels / classes\n",
    "alex_cnn = AlexNet(in_channels=1, num_classes=10)\n",
    "\n",
    "# Load pretrained weights where shapes match\n",
    "alex_cnn = load_pretrained_weights(alex_cnn, pretrained_model=pretrained_alexnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb48e1f-f781-42ca-afa8-3a515c64754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to checkpoint\n",
    "checkpoint_path = \"alexnet_cifar10.pth\"\n",
    "alex_cnn = load_pretrained_weights(alex_cnn, state_dict_path=checkpoint_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
